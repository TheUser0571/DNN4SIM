{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2 as cv\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, 'lib')\n",
    "from iplabs import IPLabViewer as viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN4SimBase(nn.Module):\n",
    "    def training_step(self, batch, loss_func=F.smooth_l1_loss):\n",
    "        images, labels = batch \n",
    "        out = self(images)            # Generate predictions\n",
    "        loss = loss_func(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, loss_func=F.smooth_l1_loss):\n",
    "        images, labels = batch \n",
    "        out = self(images)              # Generate predictions\n",
    "        loss = loss_func(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)     # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "\n",
    "        \n",
    "class CUNet(DNN4SimBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.down1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU())\n",
    "        self.down2 = nn.Sequential(nn.MaxPool2d(2, 2),\n",
    "                                   nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU())\n",
    "        self.down3 = nn.Sequential(nn.MaxPool2d(2, 2),\n",
    "                                   nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU())\n",
    "        self.down4 = nn.Sequential(nn.MaxPool2d(2, 2),\n",
    "                                   nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU())\n",
    "        self.downF = nn.Sequential(nn.MaxPool2d(2, 2),\n",
    "                                   nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2))\n",
    "        self.up1  =  nn.Sequential(nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2))\n",
    "        self.up2  =  nn.Sequential(nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2))\n",
    "        self.up3  =  nn.Sequential(nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2))\n",
    "        self.up4  =  nn.Sequential(nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=(1,1), padding_mode='reflect'),\n",
    "                                   nn.ReLU())\n",
    "        self.upF  =  nn.Sequential(nn.Conv2d(64, 1, kernel_size=1, stride=1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.downF(x4)\n",
    "        x6 = self.up1(torch.cat((x4,x5), dim=1))\n",
    "        x7 = self.up2(torch.cat((x3,x6), dim=1))\n",
    "        x8 = self.up3(torch.cat((x2,x7), dim=1))\n",
    "        x9 = self.up4(torch.cat((x1,x8), dim=1))\n",
    "        xF = self.upF(x9)                 \n",
    "        return xF\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, loss_func=F.smooth_l1_loss):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch, loss_func=loss_func) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam, loss_func=F.smooth_l1_loss):\n",
    "    print('Starting training')\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Running epoch {epoch} ... ', end='\\r')\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            print(f'Running epoch {epoch} ... {i/len(train_loader)*100:3.0f}%', end='\\r')\n",
    "            loss = model.training_step(batch, loss_func=loss_func)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(f'Running epoch {epoch} ... Done                     ', end='\\r')\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader, loss_func=loss_func)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "\n",
    "def accuracy_numpy(outputs, labels):\n",
    "    if len(outputs.shape) > 2:\n",
    "        outputs = outputs.squeeze()\n",
    "        labels = labels.squeeze()\n",
    "        acc = []\n",
    "        for i in range(outputs.shape[0]):\n",
    "            acc.append(ssim(outputs[i], labels[i], dynamic_range=max(outputs[i].max() - outputs[i].min(), labels[i].max() - labels[i].min())))\n",
    "        return np.mean(acc)\n",
    "    else:\n",
    "        return ssim(outputs, labels, dynamic_range=max(outputs.max() - outputs.min(), labels.max() - labels.min()))\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    return pytorch_ssim.ssim(outputs, labels)\n",
    "    \n",
    "def get_train_val(features, labels, train_ratio=0.8, batch_size=10):\n",
    "    if features.shape[0] != labels.shape[0]:\n",
    "        raise ValueError('Features and Labels are not of the same size')\n",
    "    if len(features.shape) != 3:\n",
    "        raise ValueError('Features and Labels should be 3-dimensional')\n",
    "\n",
    "    length = features.shape[0]\n",
    "    \n",
    "    if length % batch_size != 0:\n",
    "        features = features[:-(length % batch_size)]\n",
    "        labels = labels[:-(length % batch_size)]\n",
    "    \n",
    "    features = np.reshape(features, (-1, batch_size, 1, features.shape[1], features.shape[2]))\n",
    "    labels = np.reshape(labels, (-1, batch_size, 1, labels.shape[1], labels.shape[2]))\n",
    "    \n",
    "    n = int(train_ratio*features.shape[0])\n",
    "    \n",
    "    train_set = [(torch.FloatTensor(features[i]), torch.FloatTensor(labels[i])) for i in range(n)]\n",
    "    val_set = [(torch.FloatTensor(features[i]), torch.FloatTensor(labels[i])) for i in range(n, features.shape[0])]\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]             640\n",
      "              ReLU-2         [-1, 64, 512, 512]               0\n",
      "            Conv2d-3         [-1, 64, 512, 512]          36,928\n",
      "              ReLU-4         [-1, 64, 512, 512]               0\n",
      "         MaxPool2d-5         [-1, 64, 256, 256]               0\n",
      "            Conv2d-6        [-1, 128, 256, 256]          73,856\n",
      "              ReLU-7        [-1, 128, 256, 256]               0\n",
      "            Conv2d-8        [-1, 128, 256, 256]         147,584\n",
      "              ReLU-9        [-1, 128, 256, 256]               0\n",
      "        MaxPool2d-10        [-1, 128, 128, 128]               0\n",
      "           Conv2d-11        [-1, 256, 128, 128]         295,168\n",
      "             ReLU-12        [-1, 256, 128, 128]               0\n",
      "           Conv2d-13        [-1, 256, 128, 128]         590,080\n",
      "             ReLU-14        [-1, 256, 128, 128]               0\n",
      "        MaxPool2d-15          [-1, 256, 64, 64]               0\n",
      "           Conv2d-16          [-1, 512, 64, 64]       1,180,160\n",
      "             ReLU-17          [-1, 512, 64, 64]               0\n",
      "           Conv2d-18          [-1, 512, 64, 64]       2,359,808\n",
      "             ReLU-19          [-1, 512, 64, 64]               0\n",
      "        MaxPool2d-20          [-1, 512, 32, 32]               0\n",
      "           Conv2d-21         [-1, 1024, 32, 32]       4,719,616\n",
      "             ReLU-22         [-1, 1024, 32, 32]               0\n",
      "           Conv2d-23         [-1, 1024, 32, 32]       9,438,208\n",
      "             ReLU-24         [-1, 1024, 32, 32]               0\n",
      "  ConvTranspose2d-25          [-1, 512, 64, 64]       2,097,664\n",
      "           Conv2d-26          [-1, 512, 64, 64]       4,719,104\n",
      "             ReLU-27          [-1, 512, 64, 64]               0\n",
      "           Conv2d-28          [-1, 512, 64, 64]       2,359,808\n",
      "             ReLU-29          [-1, 512, 64, 64]               0\n",
      "  ConvTranspose2d-30        [-1, 256, 128, 128]         524,544\n",
      "           Conv2d-31        [-1, 256, 128, 128]       1,179,904\n",
      "             ReLU-32        [-1, 256, 128, 128]               0\n",
      "           Conv2d-33        [-1, 256, 128, 128]         590,080\n",
      "             ReLU-34        [-1, 256, 128, 128]               0\n",
      "  ConvTranspose2d-35        [-1, 128, 256, 256]         131,200\n",
      "           Conv2d-36        [-1, 128, 256, 256]         295,040\n",
      "             ReLU-37        [-1, 128, 256, 256]               0\n",
      "           Conv2d-38        [-1, 128, 256, 256]         147,584\n",
      "             ReLU-39        [-1, 128, 256, 256]               0\n",
      "  ConvTranspose2d-40         [-1, 64, 512, 512]          32,832\n",
      "           Conv2d-41         [-1, 64, 512, 512]          73,792\n",
      "             ReLU-42         [-1, 64, 512, 512]               0\n",
      "           Conv2d-43         [-1, 64, 512, 512]          36,928\n",
      "             ReLU-44         [-1, 64, 512, 512]               0\n",
      "           Conv2d-45          [-1, 1, 512, 512]              65\n",
      "================================================================\n",
      "Total params: 31,030,593\n",
      "Trainable params: 31,030,593\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 2254.00\n",
      "Params size (MB): 118.37\n",
      "Estimated Total Size (MB): 2373.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(CUNet(), (1, 512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test\n",
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "dataset = []\n",
    "ch = '3D-SIM 488' # Channel\n",
    "for i in range(4):\n",
    "    with open(f'DNN4SIM_data/dataset_labels_{i}_{ch}.npy', 'rb') as f:\n",
    "        dataset.append(np.load(f))\n",
    "dataset = np.concatenate(dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_ssim\n",
    "# Custom loss function combingin Smooth L1 Loss with SSIM\n",
    "def custom_loss(output, target):\n",
    "    ssim_loss = pytorch_ssim.SSIM()\n",
    "    sl1l = F.smooth_l1_loss\n",
    "    return sl1l(output, target) + 0.01 * (1 - ssim_loss(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline validation accuracy = 0.23830097913742065\n",
      "Train size = 80, Validation size = 20\n"
     ]
    }
   ],
   "source": [
    "def add_noise(img):\n",
    "    return img + np.random.random(img.shape)*0.2\n",
    "\n",
    "data_size = 100\n",
    "batch_size = 10\n",
    "\n",
    "dataset_noisy = add_noise(dataset[:data_size,:512,:512])\n",
    "\n",
    "train_set, val_set = get_train_val(dataset_noisy, dataset[:data_size,:512,:512], batch_size=batch_size)\n",
    "\n",
    "accs = []\n",
    "for im, lab in val_set:\n",
    "    accs.append(accuracy(im, lab))\n",
    "\n",
    "print(f'Baseline validation accuracy = {torch.stack(accs).mean()}')\n",
    "\n",
    "print(f'Train size = {len(train_set)*batch_size}, Validation size = {len(val_set)*batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "del dataset\n",
    "del dataset_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch [0], train_loss: 0.0206, val_loss: 0.0100, val_acc: 0.5966\n",
      "Epoch [1], train_loss: 0.0064, val_loss: 0.0034, val_acc: 0.7832\n",
      "Epoch [2], train_loss: 0.0030, val_loss: 0.0032, val_acc: 0.8237\n",
      "Epoch [3], train_loss: 0.0021, val_loss: 0.0020, val_acc: 0.8635\n",
      "Epoch [4], train_loss: 0.0017, val_loss: 0.0013, val_acc: 0.8812\n",
      "Epoch [5], train_loss: 0.0019, val_loss: 0.0011, val_acc: 0.9072\n",
      "Epoch [6], train_loss: 0.0010, val_loss: 0.0010, val_acc: 0.9165\n",
      "Epoch [7], train_loss: 0.0010, val_loss: 0.0009, val_acc: 0.9169\n",
      "Epoch [8], train_loss: 0.0009, val_loss: 0.0010, val_acc: 0.9165\n",
      "Epoch [9], train_loss: 0.0009, val_loss: 0.0009, val_acc: 0.9194\n"
     ]
    }
   ],
   "source": [
    "net = CUNet()\n",
    "history = fit(epochs=10, lr=0.001, model=net, train_loader=train_set, val_loader=val_set, loss_func=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57afb95ec3314b3bb402a5901ab842b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = [x['train_loss'] for x in history]\n",
    "val_loss = [x['val_loss'] for x in history]\n",
    "val_acc = [x['val_acc'] for x in history]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.subplot(122)\n",
    "plt.plot(val_acc)\n",
    "plt.legend(['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c1dc4a39944cfa89c696e37112fc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1d72ce1524465ca3c0f41a6bc37b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sig = 1\n",
    "view = viewer([val_set[0][1][4].numpy().squeeze(), val_set[0][0][4].numpy().squeeze(), net(val_set[0][0][4].unsqueeze(dim=0)).detach().numpy().squeeze(), cv.GaussianBlur(val_set[0][1][4].numpy().squeeze(), (0,0), sig)], title=['Original', 'Noisy', 'Net Output'], subplots=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d6b25b096a4dd9bb5e0822bf4ec826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<iplabs.IPLabViewer at 0x17669d88a88>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.close('all')\n",
    "n=11\n",
    "viewer([dataset[n], np.fft.fftshift(10*np.log10(np.abs(np.fft.fft2(dataset[n]))))], subplots=(1,2), cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
